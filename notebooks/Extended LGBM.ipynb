{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Last ditch effort, no deep-learning, but some decent features, over a year-long window: https://github.com/LenzDu/Kaggle-Competition-Favorita/blob/master/lgbm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSetPath = \"/home/ec2-user/datasets/favorita/\"\n",
    "\n",
    "StoresPath   = DataSetPath + \"stores.csv.gz\"\n",
    "ItemsPath    = DataSetPath + \"items.csv.gz\"\n",
    "OilPricePath = DataSetPath + \"oil.csv.gz\"\n",
    "HolidaysPath = DataSetPath + \"holidays_events.csv.gz\"\n",
    "Transactions = DataSetPath + \"transactions.csv.gz\"\n",
    "TrainData    = DataSetPath + \"train.csv.gz\"\n",
    "TestData     = DataSetPath + \"test.csv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import date, timedelta\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sale_amts_file=TrainData, test_sale_amts_file=TestData, items_file=ItemsPath, stores_file=StoresPath, skipRowsUpTo=66458909, minYear=2017):\n",
    "    # df_train = pd.read_feather('train_after1608_raw')\n",
    "    print (\"Loading Train data\")\n",
    "    if (skipRowsUpTo is None) or (skipRowsUpTo == 0):\n",
    "        df_train = pd.read_csv(sale_amts_file, usecols=[1, 2, 3, 4, 5], dtype={'onpromotion': bool},\n",
    "                               converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0},\n",
    "                               parse_dates=[\"date\"])\n",
    "    else:\n",
    "        df_train = pd.read_csv(sale_amts_file, usecols=[1, 2, 3, 4, 5], dtype={'onpromotion': bool},\n",
    "                               converters={'unit_sales': lambda u: np.log1p(float(u)) if float(u) > 0 else 0},\n",
    "                               parse_dates=[\"date\"], skiprows=range(1,skipRowsUpTo))\n",
    "    \n",
    "    for year in range(2013,2018,1):\n",
    "        print (\"adding zero entry for year \" + str(year))\n",
    "        df_train.loc[len(df_train)] = [ \\\n",
    "            pd.Timestamp(str(year) + '-12-25 00:00:00'),\n",
    "            1,\n",
    "            1,\n",
    "            0,\n",
    "            False\n",
    "    ]\n",
    "    \n",
    "    df_test = pd.read_csv(test_sale_amts_file, usecols=[0, 1, 2, 3, 4], dtype={'onpromotion': bool},\n",
    "                          parse_dates=[\"date\"]).set_index(['store_nbr', 'item_nbr', 'date'])\n",
    "\n",
    "    # subset data\n",
    "    df_2017 = df_train.loc[df_train.date>=pd.datetime(minYear,1,1)]\n",
    "\n",
    "    # promo\n",
    "    promo_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)\n",
    "    promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\n",
    "    promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\n",
    "    promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\n",
    "    promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\n",
    "    promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "    del promo_2017_test, promo_2017_train\n",
    "\n",
    "    df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)\n",
    "    df_2017.columns = df_2017.columns.get_level_values(1)\n",
    "\n",
    "    # items\n",
    "    items = pd.read_csv(items_file).set_index(\"item_nbr\")\n",
    "    stores = pd.read_csv(stores_file).set_index(\"store_nbr\")\n",
    "    # items = items.reindex(df_2017.index.get_level_values(1))\n",
    "\n",
    "    return df_train, df_2017, promo_2017, items, stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Train data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2903: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding zero entry for year 2013\n",
      "adding zero entry for year 2014\n",
      "adding zero entry for year 2015\n",
      "adding zero entry for year 2016\n",
      "adding zero entry for year 2017\n"
     ]
    }
   ],
   "source": [
    "df, df_unstacked, promo_df, items, stores = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125497035</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2089339</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497036</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2106464</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497037</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2110456</td>\n",
       "      <td>5.262690</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497038</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2113914</td>\n",
       "      <td>5.293305</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497039</th>\n",
       "      <td>2017-08-15</td>\n",
       "      <td>54</td>\n",
       "      <td>2116416</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497040</th>\n",
       "      <td>2013-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497041</th>\n",
       "      <td>2014-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497042</th>\n",
       "      <td>2015-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497043</th>\n",
       "      <td>2016-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125497044</th>\n",
       "      <td>2017-12-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date  store_nbr  item_nbr  unit_sales onpromotion\n",
       "125497035 2017-08-15         54   2089339    1.609438       False\n",
       "125497036 2017-08-15         54   2106464    0.693147        True\n",
       "125497037 2017-08-15         54   2110456    5.262690       False\n",
       "125497038 2017-08-15         54   2113914    5.293305        True\n",
       "125497039 2017-08-15         54   2116416    1.098612       False\n",
       "125497040 2013-12-25          1         1    0.000000       False\n",
       "125497041 2014-12-25          1         1    0.000000       False\n",
       "125497042 2015-12-25          1         1    0.000000       False\n",
       "125497043 2016-12-25          1         1    0.000000       False\n",
       "125497044 2017-12-25          1         1    0.000000       False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[-10:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017, promo_2017 = df_unstacked, promo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "promo_2017 = promo_2017[df_2017[pd.date_range(date(2017,1,1), date(2017,8,15))].max(axis=1)>0]\n",
    "df_2017 = df_2017[df_2017[pd.date_range(date(2017,1,1), date(2017,8,15))].max(axis=1)>0]\n",
    "promo_2017 = promo_2017.astype('int')\n",
    "df_test = pd.read_csv(TestData, usecols=[0, 1, 2, 3, 4], dtype={'onpromotion': bool},\n",
    "                      parse_dates=[\"date\"]).set_index(['store_nbr', 'item_nbr', 'date'])\n",
    "item_nbr_test = df_test.index.get_level_values(1)\n",
    "item_nbr_train = df_2017.index.get_level_values(1)\n",
    "item_inter = list(set(item_nbr_train).intersection(set(item_nbr_test)))\n",
    "\n",
    "df_2017 = df_2017.loc[df_2017.index.get_level_values(1).isin(item_inter)]\n",
    "promo_2017 = promo_2017.loc[promo_2017.index.get_level_values(1).isin(item_inter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]\n",
    "\n",
    "def prepare_dataset(t2017, is_train=True, one_hot=False):\n",
    "    X = pd.DataFrame({\n",
    "        \"day_1_2017\": get_timespan(df_2017, t2017, 1, 1).values.ravel(),\n",
    "        \"day_2_2017\": get_timespan(df_2017, t2017, 2, 1).values.ravel(),\n",
    "        \"day_3_2017\": get_timespan(df_2017, t2017, 3, 1).values.ravel(),\n",
    "#         \"day_4_2017\": get_timespan(df_2017, t2017, 4, 1).values.ravel(),\n",
    "#         \"day_5_2017\": get_timespan(df_2017, t2017, 5, 1).values.ravel(),\n",
    "#         \"day_6_2017\": get_timespan(df_2017, t2017, 6, 1).values.ravel(),\n",
    "#         \"day_7_2017\": get_timespan(df_2017, t2017, 7, 1).values.ravel(),\n",
    "#         \"mean_3_2017\": get_timespan(df_2017, t2017, 3, 3).mean(axis=1).values,\n",
    "#         \"std_7_2017\": get_timespan(df_2017, t2017, 7, 7).std(axis=1).values,\n",
    "#         \"max_7_2017\": get_timespan(df_2017, t2017, 7, 7).max(axis=1).values,\n",
    "#         \"median_7_2017\": get_timespan(df_2017, t2017, 7, 7).median(axis=1).values,\n",
    "#         \"median_30_2017\": get_timespan(df_2017, t2017, 30, 30).median(axis=1).values,\n",
    "#         \"median_140_2017\": get_timespan(df_2017, t2017, 140, 140).median(axis=1).values,\n",
    "        'promo_3_2017': get_timespan(promo_2017, t2017, 3, 3).sum(axis=1).values,\n",
    "        \"last_year_mean\": get_timespan(df_2017, t2017, 365, 16).mean(axis=1).values,\n",
    "        \"last_year_count0\": (get_timespan(df_2017, t2017, 365, 16)==0).sum(axis=1).values,\n",
    "        \"last_year_promo\": get_timespan(promo_2017, t2017, 365, 16).sum(axis=1).values\n",
    "    })\n",
    "    \n",
    "    for i in [7, 14, 21, 30, 60, 90, 140, 365]:\n",
    "        X['mean_{}_2017'.format(i)] = get_timespan(df_2017, t2017, i, i).mean(axis=1).values\n",
    "        X['median_{}_2017'.format(i)] = get_timespan(df_2017, t2017, i, i).mean(axis=1).values\n",
    "        X['max_{}_2017'.format(i)] = get_timespan(df_2017, t2017, i, i).max(axis=1).values\n",
    "        X['mean_{}_haspromo_2017'.format(i)] = get_timespan(df_2017, t2017, i, i)[get_timespan(promo_2017, t2017, i, i)==1].mean(axis=1).values\n",
    "        X['mean_{}_nopromo_2017'.format(i)] = get_timespan(df_2017, t2017, i, i)[get_timespan(promo_2017, t2017, i, i)==0].mean(axis=1).values\n",
    "        X['count0_{}_2017'.format(i)] = (get_timespan(df_2017, t2017, i, i)==0).sum(axis=1).values\n",
    "        X['promo_{}_2017'.format(i)] = get_timespan(promo_2017, t2017, i, i).sum(axis=1).values\n",
    "        item_mean = get_timespan(df_2017, t2017, i, i).mean(axis=1).groupby('item_nbr').mean().to_frame('item_mean')\n",
    "        X['item_{}_mean'.format(i)] = df_2017.join(item_mean)['item_mean'].values\n",
    "        item_count0 = (get_timespan(df_2017, t2017, i, i)==0).sum(axis=1).groupby('item_nbr').mean().to_frame('item_count0')\n",
    "        X['item_{}_count0_mean'.format(i)] = df_2017.join(item_count0)['item_count0'].values\n",
    "        store_mean = get_timespan(df_2017, t2017, i, i).mean(axis=1).groupby('store_nbr').mean().to_frame('store_mean')\n",
    "        X['store_{}_mean'.format(i)] = df_2017.join(store_mean)['store_mean'].values\n",
    "        store_count0 = (get_timespan(df_2017, t2017, i, i)==0).sum(axis=1).groupby('store_nbr').mean().to_frame('store_count0')\n",
    "        X['store_{}_count0_mean'.format(i)] = df_2017.join(store_count0)['store_count0'].values\n",
    "        \n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_10_dow{}'.format(i)] = get_timespan(df_2017, t2017, 70-i, 10, freq='7D').mean(axis=1).values\n",
    "        X['count0_10_dow{}'.format(i)] = (get_timespan(df_2017, t2017, 70-i, 10)==0).sum(axis=1).values\n",
    "        X['promo_10_dow{}'.format(i)] = get_timespan(promo_2017, t2017, 70-i, 10, freq='7D').sum(axis=1).values\n",
    "        item_mean = get_timespan(df_2017, t2017, 70-i, 10, freq='7D').mean(axis=1).groupby('item_nbr').mean().to_frame('item_mean')\n",
    "        X['item_mean_10_dow{}'.format(i)] = df_2017.join(item_mean)['item_mean'].values\n",
    "        X['mean_20_dow{}'.format(i)] = get_timespan(df_2017, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "        \n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[t2017 + timedelta(days=i)].values\n",
    "    \n",
    "    if one_hot:\n",
    "        family_dummy = pd.get_dummies(df_2017.join(items)['family'], prefix='family')\n",
    "        X = pd.concat([X, family_dummy.reset_index(drop=True)], axis=1)\n",
    "        store_dummy = pd.get_dummies(df_2017.reset_index().store_nbr, prefix='store')\n",
    "        X = pd.concat([X, store_dummy.reset_index(drop=True)], axis=1)\n",
    "#         X['family_count'] = df_2017.join(items).groupby('family').count().iloc[:,0].values\n",
    "#         X['store_count'] = df_2017.reset_index().groupby('family').count().iloc[:,0].values\n",
    "    else:\n",
    "        df_items = df_2017.join(items)\n",
    "        df_stores = df_2017.join(stores)\n",
    "        X['family'] = df_items['family'].astype('category').cat.codes.values\n",
    "        X['perish'] = df_items['perishable'].values\n",
    "        X['item_class'] = df_items['class'].values\n",
    "        X['store_nbr'] = df_2017.reset_index().store_nbr.values\n",
    "        X['store_cluster'] = df_stores['cluster'].values\n",
    "        X['store_type'] = df_stores['type'].astype('category').cat.codes.values\n",
    "#     X['item_nbr'] = df_2017.reset_index().item_nbr.values\n",
    "#     X['item_mean'] = df_2017.join(item_mean)['item_mean']\n",
    "#     X['store_mean'] = df_2017.join(store_mean)['store_mean']\n",
    "\n",
    "#     store_promo_90_mean = get_timespan(promo_2017, t2017, 90, 90).sum(axis=1).groupby('store_nbr').mean().to_frame('store_promo_90_mean')\n",
    "#     X['store_promo_90_mean'] = df_2017.join(store_promo_90_mean)['store_promo_90_mean'].values\n",
    "#     item_promo_90_mean = get_timespan(promo_2017, t2017, 90, 90).sum(axis=1).groupby('item_nbr').mean().to_frame('item_promo_90_mean')\n",
    "#     X['item_promo_90_mean'] = df_2017.join(item_promo_90_mean)['item_promo_90_mean'].values\n",
    "    \n",
    "    if is_train:\n",
    "        y = df_2017[pd.date_range(t2017, periods=16)].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "0.."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/nanops.py:359: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  the_mean = the_sum / count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1..2..3..4..5..6..7..8..9..10..11..12..13.."
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "X_l, y_l = [], []\n",
    "t2017 = date(2017, 7, 5)\n",
    "n_range = 14\n",
    "for i in range(n_range):\n",
    "    print(i, end='..')\n",
    "    delta = timedelta(days=7 * i)\n",
    "    X_tmp, y_tmp = prepare_dataset(t2017 - delta)\n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)\n",
    "    \n",
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l\n",
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 31,\n",
    "    'objective': 'regression',\n",
    "    'min_data_in_leaf': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'metric': 'l2',\n",
    "    'max_bin':128,\n",
    "    'num_threads': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting models...\n",
      "Step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l2: 0.310161\tvalid_1's l2: 0.299719\n",
      "[200]\ttraining's l2: 0.304532\tvalid_1's l2: 0.295942\n",
      "[300]\ttraining's l2: 0.301856\tvalid_1's l2: 0.294748\n",
      "[400]\ttraining's l2: 0.299955\tvalid_1's l2: 0.293998\n",
      "[500]\ttraining's l2: 0.298409\tvalid_1's l2: 0.29361\n",
      "[600]\ttraining's l2: 0.297004\tvalid_1's l2: 0.293228\n",
      "[700]\ttraining's l2: 0.295768\tvalid_1's l2: 0.292995\n",
      "mean_7_2017: 5204838.01\n",
      "mean_14_2017: 3293179.93\n",
      "median_7_2017: 3169940.48\n",
      "day_1_2017: 476458.28\n",
      "promo_0: 421472.25\n",
      "median_14_2017: 339539.24\n",
      "mean_21_2017: 294412.09\n",
      "mean_20_dow0: 228421.80\n",
      "mean_10_dow0: 220998.99\n",
      "mean_30_2017: 176374.80\n",
      "mean_7_nopromo_2017: 144680.56\n",
      "mean_30_nopromo_2017: 118666.98\n",
      "store_nbr: 104625.95\n",
      "day_2_2017: 84589.79\n",
      "promo_10_dow0: 66507.41\n",
      "Step 2\n",
      "[100]\ttraining's l2: 0.339544\tvalid_1's l2: 0.337121\n",
      "[200]\ttraining's l2: 0.3332\tvalid_1's l2: 0.332462\n",
      "[300]\ttraining's l2: 0.329854\tvalid_1's l2: 0.330908\n",
      "[400]\ttraining's l2: 0.327408\tvalid_1's l2: 0.330143\n",
      "[500]\ttraining's l2: 0.325448\tvalid_1's l2: 0.329645\n",
      "[600]\ttraining's l2: 0.323837\tvalid_1's l2: 0.329242\n",
      "[700]\ttraining's l2: 0.322434\tvalid_1's l2: 0.329114\n",
      "mean_14_2017: 4477611.99\n",
      "mean_7_2017: 3710708.65\n",
      "median_7_2017: 1657341.52\n",
      "mean_30_nopromo_2017: 338412.36\n",
      "mean_30_2017: 301882.41\n",
      "promo_1: 278439.40\n",
      "mean_20_dow1: 191085.14\n",
      "mean_10_dow1: 190215.03\n",
      "mean_21_2017: 166187.83\n",
      "day_1_2017: 165430.86\n",
      "median_14_2017: 96553.11\n",
      "mean_60_nopromo_2017: 87302.62\n",
      "mean_7_nopromo_2017: 85949.37\n",
      "store_nbr: 70951.48\n",
      "mean_90_nopromo_2017: 36168.19\n",
      "Step 3\n",
      "[100]\ttraining's l2: 0.343652\tvalid_1's l2: 0.351627\n",
      "[200]\ttraining's l2: 0.334396\tvalid_1's l2: 0.347639\n",
      "[300]\ttraining's l2: 0.33012\tvalid_1's l2: 0.346542\n",
      "[400]\ttraining's l2: 0.327324\tvalid_1's l2: 0.346062\n",
      "[500]\ttraining's l2: 0.325048\tvalid_1's l2: 0.345325\n",
      "[600]\ttraining's l2: 0.323271\tvalid_1's l2: 0.344842\n",
      "[700]\ttraining's l2: 0.321661\tvalid_1's l2: 0.344473\n",
      "mean_14_2017: 5908991.78\n",
      "mean_7_2017: 3067782.63\n",
      "median_7_2017: 1073645.77\n",
      "mean_10_dow2: 831191.97\n",
      "mean_30_2017: 555218.14\n",
      "mean_20_dow2: 490174.21\n",
      "promo_2: 406576.66\n",
      "mean_30_nopromo_2017: 275682.68\n",
      "mean_4_dow2: 231964.97\n",
      "median_21_2017: 222700.71\n",
      "median_30_2017: 178071.20\n",
      "mean_7_nopromo_2017: 127838.91\n",
      "family: 96708.82\n",
      "store_nbr: 87875.74\n",
      "max_7_2017: 86988.64\n",
      "Step 4\n",
      "[100]\ttraining's l2: 0.36105\tvalid_1's l2: 0.363115\n",
      "[200]\ttraining's l2: 0.351728\tvalid_1's l2: 0.360107\n",
      "[300]\ttraining's l2: 0.347076\tvalid_1's l2: 0.359613\n",
      "[400]\ttraining's l2: 0.343849\tvalid_1's l2: 0.359667\n",
      "[500]\ttraining's l2: 0.341416\tvalid_1's l2: 0.359587\n",
      "[600]\ttraining's l2: 0.339323\tvalid_1's l2: 0.359424\n",
      "[700]\ttraining's l2: 0.337543\tvalid_1's l2: 0.359064\n",
      "mean_14_2017: 7881678.04\n",
      "mean_7_2017: 2736568.91\n",
      "median_7_2017: 1278943.31\n",
      "mean_30_2017: 650811.66\n",
      "mean_10_dow3: 615009.84\n",
      "mean_30_nopromo_2017: 483208.65\n",
      "mean_20_dow3: 443634.99\n",
      "mean_4_dow3: 383432.39\n",
      "promo_3: 303936.98\n",
      "mean_21_2017: 299931.93\n",
      "median_14_2017: 252816.43\n",
      "mean_60_nopromo_2017: 115412.11\n",
      "store_nbr: 103056.70\n",
      "mean_7_nopromo_2017: 96515.35\n",
      "max_7_2017: 89739.97\n",
      "Step 5\n",
      "[100]\ttraining's l2: 0.368813\tvalid_1's l2: 0.365484\n",
      "[200]\ttraining's l2: 0.358414\tvalid_1's l2: 0.360437\n",
      "[300]\ttraining's l2: 0.353194\tvalid_1's l2: 0.359379\n",
      "[400]\ttraining's l2: 0.349513\tvalid_1's l2: 0.358998\n",
      "[500]\ttraining's l2: 0.346576\tvalid_1's l2: 0.358952\n",
      "[600]\ttraining's l2: 0.344201\tvalid_1's l2: 0.358947\n",
      "[700]\ttraining's l2: 0.342134\tvalid_1's l2: 0.359164\n",
      "mean_14_2017: 5808880.89\n",
      "mean_4_dow4: 3671067.20\n",
      "mean_7_2017: 1923214.24\n",
      "mean_10_dow4: 1587990.84\n",
      "median_7_2017: 1317785.35\n",
      "mean_20_dow4: 614001.24\n",
      "median_14_2017: 509209.29\n",
      "promo_4: 304074.28\n",
      "mean_30_nopromo_2017: 251900.91\n",
      "mean_21_2017: 183407.03\n",
      "store_nbr: 178831.62\n",
      "mean_30_2017: 118230.25\n",
      "day_3_2017: 110646.28\n",
      "day_2_2017: 97649.40\n",
      "day_1_2017: 90485.21\n",
      "Step 6\n",
      "[100]\ttraining's l2: 0.373221\tvalid_1's l2: 0.373438\n",
      "[200]\ttraining's l2: 0.360574\tvalid_1's l2: 0.368302\n",
      "[300]\ttraining's l2: 0.355131\tvalid_1's l2: 0.367536\n",
      "[400]\ttraining's l2: 0.351819\tvalid_1's l2: 0.366809\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting models...\")\n",
    "MAX_ROUNDS = 700\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "best_rounds = []\n",
    "cate_vars = ['family', 'perish', 'store_nbr', 'store_cluster', 'store_type']\n",
    "w = (X_val[\"perish\"] * 0.25 + 1) / (X_val[\"perish\"] * 0.25 + 1).mean()\n",
    "\n",
    "for i in range(16):\n",
    "\n",
    "    print(\"Step %d\" % (i+1))\n",
    "\n",
    "    dtrain = lgb.Dataset(\n",
    "        X_train, label=y_train[:, i],\n",
    "        categorical_feature=cate_vars,\n",
    "        weight=None)\n",
    "    dval = lgb.Dataset(\n",
    "        X_val, label=y_val[:, i], reference=dtrain,\n",
    "        weight=w,\n",
    "        categorical_feature=cate_vars)\n",
    "    bst = lgb.train(\n",
    "        params, dtrain, num_boost_round=MAX_ROUNDS,\n",
    "        valid_sets=[dtrain, dval], verbose_eval=100)\n",
    "\n",
    "    print(\"\\n\".join((\"%s: %.2f\" % x) for x in sorted(\n",
    "        zip(X_train.columns, bst.feature_importance(\"gain\")),\n",
    "        key=lambda x: x[1], reverse=True)[:15]))\n",
    "    best_rounds.append(bst.best_iteration or MAX_ROUNDS)\n",
    "\n",
    "    val_pred.append(bst.predict(X_val, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    test_pred.append(bst.predict(X_test, num_iteration=bst.best_iteration or MAX_ROUNDS))\n",
    "    gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
